{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import tqdm\n",
    "import math\n",
    "import librosa\n",
    "import librosa.display as display\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "import IPython.display as i_disn\n",
    "%matplotlib inline\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "mlp.rc(\"xtick\",labelsize=12)\n",
    "mlp.rc(\"ytick\",labelsize=12)\n",
    "mlp.rc(\"axes\",labelsize=14)\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution, enable_eager_execution\n",
    "\n",
    "disable_eager_execution()\n",
    "# enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"X_train_features.pkl\",\"rb\") as file:\n",
    "    X_train_features = pickle.load(file)\n",
    "    \n",
    "with open(r\"X_test_features.pkl\",\"rb\") as file:\n",
    "    X_test_features = pickle.load(file)\n",
    "    \n",
    "with open(r\"X_val_features.pkl\",\"rb\") as file:\n",
    "    X_val_features = pickle.load(file)\n",
    "\n",
    "with open(r\"y_train.pkl\",\"rb\") as file:\n",
    "    y_train = pickle.load(file)\n",
    "\n",
    "with open(r\"y_test.pkl\",\"rb\") as file:\n",
    "    y_test = pickle.load(file)\n",
    "\n",
    "with open(r\"y_val.pkl\",\"rb\") as file:\n",
    "    y_val = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data:  (22840, 257, 69, 1)\n",
      "Shape of test data:  (2820, 257, 69, 1)\n",
      "Shape of validation data:  (2538, 257, 69, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training data: \", X_train_features.shape)\n",
    "print(\"Shape of test data: \", X_test_features.shape)\n",
    "print(\"Shape of validation data: \", X_val_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.6829035 ],\n",
       "        [0.7642675 ],\n",
       "        [0.67142975],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.6792315 ],\n",
       "        [0.7478292 ],\n",
       "        [0.7216569 ],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.6563965 ],\n",
       "        [0.6257569 ],\n",
       "        [0.5951136 ],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_features[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(features):\n",
    "    scaled = []\n",
    "    min_max_values = []\n",
    "    for f in features:\n",
    "        min_val = np.min(f)\n",
    "        max_val = np.max(f)\n",
    "        f_normalized = (f - min_val) / (max_val - min_val)\n",
    "        min_max_values.append((min_val,max_val))\n",
    "        scaled.append(f_normalized)\n",
    "    return np.array(scaled),np.array(min_max_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_features, _  = MinMaxScaler(X_train_features)\n",
    "# X_val_features , _ = MinMaxScaler(X_val_features)\n",
    "# X_test_features , _= MinMaxScaler(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features, X_train_min_max  = MinMaxScaler(X_train_features)\n",
    "X_val_features , X_val_min_max = MinMaxScaler(X_val_features)\n",
    "X_test_features , X_test_min_max = MinMaxScaler(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxUnScaler(features, min_max_values):\n",
    "    unscaled = []\n",
    "    for f,min_max in zip(features,min_max_values):\n",
    "        unscaled_feature = (f * (min_max[1] - min_max[0])) + min_max[0]\n",
    "        unscaled.append(unscaled_feature)\n",
    "    return np.array(unscaled_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unscaled = MinMaxUnScaler(X_train_features,X_train_min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.6829035 ],\n",
       "        [0.7642675 ],\n",
       "        [0.67142975],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.6792315 ],\n",
       "        [0.7478292 ],\n",
       "        [0.7216569 ],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.6563965 ],\n",
       "        [0.6257569 ],\n",
       "        [0.5951136 ],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_features[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-45.258553 ],\n",
       "       [-50.930992 ],\n",
       "       [-56.96792  ],\n",
       "       [-59.533783 ],\n",
       "       [-58.934425 ],\n",
       "       [-62.117554 ],\n",
       "       [-58.834568 ],\n",
       "       [-51.464764 ],\n",
       "       [-49.882156 ],\n",
       "       [-37.038    ],\n",
       "       [-47.772446 ],\n",
       "       [-44.34912  ],\n",
       "       [-50.97335  ],\n",
       "       [-40.37213  ],\n",
       "       [-43.60937  ],\n",
       "       [-68.1235   ],\n",
       "       [-34.536587 ],\n",
       "       [-16.81588  ],\n",
       "       [ -4.2324905],\n",
       "       [ -4.2877045],\n",
       "       [ -2.368332 ],\n",
       "       [ -2.5424194],\n",
       "       [ -5.567108 ],\n",
       "       [ -7.2952576],\n",
       "       [ -5.4086075],\n",
       "       [ -4.6268005],\n",
       "       [ -5.474991 ],\n",
       "       [ -7.591919 ],\n",
       "       [ -8.63797  ],\n",
       "       [ -8.500275 ],\n",
       "       [ -7.799553 ],\n",
       "       [ -6.982849 ],\n",
       "       [ -6.3534164],\n",
       "       [ -6.073807 ],\n",
       "       [ -5.983444 ],\n",
       "       [ -6.2961884],\n",
       "       [ -7.132103 ],\n",
       "       [ -7.529915 ],\n",
       "       [ -7.4950867],\n",
       "       [ -8.007965 ],\n",
       "       [-10.673752 ],\n",
       "       [-14.893188 ],\n",
       "       [-17.6651   ],\n",
       "       [-17.865585 ],\n",
       "       [-27.68695  ],\n",
       "       [-30.76886  ],\n",
       "       [-22.717491 ],\n",
       "       [-25.164825 ],\n",
       "       [-29.800652 ],\n",
       "       [-34.76894  ],\n",
       "       [-40.119125 ],\n",
       "       [-35.772938 ],\n",
       "       [-44.602936 ],\n",
       "       [-80.       ],\n",
       "       [-80.       ],\n",
       "       [-80.       ],\n",
       "       [-80.       ],\n",
       "       [-80.       ],\n",
       "       [-80.       ],\n",
       "       [-80.       ],\n",
       "       [-80.       ],\n",
       "       [-80.       ],\n",
       "       [-80.       ],\n",
       "       [-80.       ],\n",
       "       [-80.       ],\n",
       "       [-80.       ],\n",
       "       [-80.       ],\n",
       "       [-80.       ],\n",
       "       [-80.       ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_unscaled[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data:  (22840, 257, 69, 1)\n",
      "Shape of test data:  (2820, 257, 69, 1)\n",
      "Shape of validation data:  (2538, 257, 69, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training data: \", X_train_features.shape)\n",
    "print(\"Shape of test data: \", X_test_features.shape)\n",
    "print(\"Shape of validation data: \", X_val_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train =  X_train_features.reshape(len(X_train_features),np.prod(X_train_features.shape[1:]))\n",
    "X_test = X_test_features.reshape(len(X_test_features),np.prod(X_test_features.shape[1:]))\n",
    "X_val = X_val_features.reshape(len(X_val_features),np.prod(X_val_features.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data:  (22840, 17733)\n",
      "Shape of test data:  (2820, 17733)\n",
      "Shape of validation data:  (2538, 17733)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training data: \", X_train.shape)\n",
    "print(\"Shape of test data: \", X_test.shape)\n",
    "print(\"Shape of validation data: \", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rabinnepal/gitHub/Neural Network Project/codes'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XXX lineno: 14, opcode: 47\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "unknown opcode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/rabinnepal/gitHub/Neural Network Project/codes/analyse_model.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rabinnepal/gitHub/Neural%20Network%20Project/codes/analyse_model.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vae \u001b[39m=\u001b[39m load_model(\u001b[39m\"\u001b[39;49m\u001b[39m50_epoch.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/nn_project_env/lib/python3.10/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[39mreturn\u001b[39;00m saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[39m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[39m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[39m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m    263\u001b[0m     filepath, custom_objects\u001b[39m=\u001b[39;49mcustom_objects, \u001b[39mcompile\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcompile\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    264\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/nn_project_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32mC:/Users/rnepal/AppData/Local/Temp/ipykernel_13808/2801134451.py:14\u001b[0m, in \u001b[0;36msampling\u001b[0;34m(args)\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: unknown opcode"
     ]
    }
   ],
   "source": [
    "vae = load_model(\"50_epoch.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_dim = X_train.shape[1]\n",
    "latent_dim = 2  # Size of the latent space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder network\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "x = Dense(4096, activation='relu')(inputs)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# Reparameterization trick to sample from the latent space\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch_size = K.shape(z_mean)[0]\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1.)\n",
    "    return z_mean #+ K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# Decoder network\n",
    "decoder_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "x = Dense(256, activation='relu')(decoder_inputs)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "outputs = Dense(17733, activation='sigmoid')(x)  # Output layer with sigmoid activation for MNIST\n",
    "\n",
    "# Define the encoder and decoder models\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder = Model(decoder_inputs, outputs, name='decoder')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see encoder summary\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see decoder summary\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function for VAE\n",
    "def vae_loss(inputs, x_decoded_mean):\n",
    "    recon_loss = original_dim * binary_crossentropy(inputs, x_decoded_mean)\n",
    "    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return K.mean(recon_loss + kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "dense_vae = Model(inputs, outputs, name='vae')\n",
    "dense_vae.compile(optimizer='adam', loss=vae_loss)\n",
    "dense_vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TestLossCallback(Callback):\n",
    "#     def __init__(self, test_data):\n",
    "#         self.test_data = test_data\n",
    "#         self.test_losses = []\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         test_loss = self.model.evaluate(self.test_data, self.test_data, verbose=0)\n",
    "#         print(f\"\\nTest Loss after Epoch {epoch + 1}: {test_loss}\")\n",
    "#         self.test_losses.append(test_loss)\n",
    "\n",
    "# test_loss_callback = TestLossCallback(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Train the VAE\n",
    "\n",
    "# dense_vae_history = dense_vae.fit(X_train, X_train, epochs=10, batch_size=128, shuffle=True, validation_data=(X_val, X_val),callbacks=[test_loss_callback])\n",
    "\n",
    "#  Train the VAE\n",
    "\n",
    "dense_vae_history = dense_vae.fit(X_train, X_train, epochs=100, batch_size=128, shuffle=True, validation_data=(X_val, X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dense_vae_history.history[\"loss\"])\n",
    "plt.plot(dense_vae_history.history[\"val_loss\"])\n",
    "plt.legend([\"Loss\", \"Validation Loss\"])\n",
    "plt.title(\"Loss plot for VAE with MLP\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss = dense_vae.evaluate(X_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023-11-15 14:06:58.386379: W tensorflow/c/c_api.cc:305] Operation '{name:'loss/mul' id:342 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/decoder_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
    "22840/22840 [==============================] - 64s 3ms/sample - loss: 9949928.5254 - val_loss: 8427.6236\n",
    "Epoch 2/100\n",
    "22840/22840 [==============================] - 62s 3ms/sample - loss: 8685.8543 - val_loss: 8569.1816\n",
    "Epoch 3/100\n",
    "22840/22840 [==============================] - 61s 3ms/sample - loss: 8522.9825 - val_loss: 8377.5673\n",
    "Epoch 4/100\n",
    "22840/22840 [==============================] - 61s 3ms/sample - loss: 8506.5232 - val_loss: 8205.4903\n",
    "Epoch 5/100\n",
    "22840/22840 [==============================] - 61s 3ms/sample - loss: 8261.0748 - val_loss: 8104.8371\n",
    "Epoch 6/100\n",
    "22840/22840 [==============================] - 204s 9ms/sample - loss: 12601.6746 - val_loss: 9356.3297\n",
    "Epoch 7/100\n",
    "22840/22840 [==============================] - 151s 7ms/sample - loss: 17122.5691 - val_loss: 10473.3295\n",
    "Epoch 8/100\n",
    "22840/22840 [==============================] - 95s 4ms/sample - loss: 9971.8597 - val_loss: 9464.1363\n",
    "Epoch 9/100\n",
    "22840/22840 [==============================] - 123s 5ms/sample - loss: 9107.4455 - val_loss: 8522.2567\n",
    "Epoch 10/100\n",
    "22840/22840 [==============================] - 61s 3ms/sample - loss: 11377.3423 - val_loss: 11114.9137\n",
    "Epoch 11/100\n",
    "22840/22840 [==============================] - 62s 3ms/sample - loss: 11209.2646 - val_loss: 10870.3877\n",
    "Epoch 12/100\n",
    "22840/22840 [==============================] - 62s 3ms/sample - loss: 10961.6752 - val_loss: 10756.3944\n",
    "Epoch 13/100\n",
    "22840/22840 [==============================] - 61s 3ms/sample - loss: 10430.8617 - val_loss: 10006.0978"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and visualize reconstructed samples\n",
    "decoded_imgs = dense_vae.predict(X_test)\n",
    "n = 1 # Number of samples to visualize\n",
    "# plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Original images\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(69, 257))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Reconstructed images\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(69, 257))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = dense_vae.predict(X_train[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(X_test[0].reshape(257, 69), sr=22050, x_axis='time', y_axis='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(decoded_imgs[0].reshape(257, 69), sr=22050, x_axis='time', y_axis='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = librosa.istft(decoded_imgs[0].reshape(257, 69))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(data=abc,rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Lambda, Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Convolutional VAE architecture\n",
    "input_shape = (1025,81,1)\n",
    "latent_dim = 2  # Size of the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder network\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256,activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(x)\n",
    "z_log_var = Dense(latent_dim)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    # return the z mean\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim), mean=0.0, stddev=1.0)\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Lambda(sampling)([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs, z)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(latent_dim,))\n",
    "y = Dense(1025 * 81 * 64, activation='relu')(decoder_input)\n",
    "y = Reshape((1025, 81, 64))(y)\n",
    "# y = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(y)\n",
    "y = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(y)\n",
    "y = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = y\n",
    "decoder = Model(decoder_input,y)\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = decoder(encoder(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function for VAE\n",
    "def vae_loss(inputs, outputs):\n",
    "    xent_loss = K.sum(K.binary_crossentropy(inputs, outputs), axis=(1, 2, 3))\n",
    "    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return K.mean(xent_loss + kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_vae = Model(inputs, outputs)\n",
    "cnn_vae.compile(optimizer='adam', loss=vae_loss)\n",
    "cnn_vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the VAE\n",
    "cnn_vae_history = cnn_vae.fit(X_train_features, X_train_features, epochs=2, batch_size=128, shuffle=True, validation_data=(X_val_features, X_val_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
